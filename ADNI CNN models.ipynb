{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62739cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''2D CNNs'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256d279b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import layers\n",
    "from keras import models\n",
    "from keras import optimizers\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75ed639",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir=r'~\\preproc_2d_data\\train'\n",
    "validation_dir=r'~\\preproc_2d_data\\validation'\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "train_generator = train_datagen.flow_from_directory(train_dir,target_size=(64, 64),batch_size=20,class_mode='categorical',shuffle=True,seed=100)\n",
    "validation_generator = test_datagen.flow_from_directory(validation_dir,target_size=(64, 64),batch_size=20,class_mode='categorical',shuffle=True,seed=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1343d40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu',\n",
    "input_shape=(64, 64, 3)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(512, activation='relu'))\n",
    "model.add(layers.Dense(2, activation='softmax'))\n",
    "\n",
    "opt = keras.optimizers.SGD(learning_rate=1e-2)\n",
    "model.compile(loss='binary_crossentropy',optimizer=opt,metrics=['acc','AUC'])\n",
    "\n",
    "# callbacks\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\n",
    "    r\"~\\models\\3convlr1e-2.h5\", save_best_only=True\n",
    ")\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(monitor=\"val_acc\", patience=15)\n",
    "\n",
    "reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
    "                              patience=5, min_lr=1e-4)\n",
    "\n",
    "start_time = time.time()\n",
    "history = model.fit_generator(train_generator,steps_per_epoch=100,epochs=80,validation_data=validation_generator,validation_steps=50,shuffle=True,callbacks=[checkpoint_cb, early_stopping_cb,reduce_lr],)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4a388f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu',\n",
    "input_shape=(64, 64, 3)))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(512, activation='relu'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Dense(2, activation='softmax'))\n",
    "\n",
    "opt = keras.optimizers.SGD(learning_rate=1e-2)\n",
    "model.compile(loss='binary_crossentropy',optimizer=opt,metrics=['acc','AUC'])\n",
    "\n",
    "# callbacks\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\n",
    "    r\"~\\models\\3convBNlr1e-2.h5\", save_best_only=True\n",
    ")\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(monitor=\"val_acc\", patience=15)\n",
    "\n",
    "reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
    "                              patience=5, min_lr=1e-4)\n",
    "\n",
    "start_time = time.time()\n",
    "history = model.fit_generator(train_generator,steps_per_epoch=100,epochs=80,validation_data=validation_generator,validation_steps=50,shuffle=True,callbacks=[checkpoint_cb, early_stopping_cb,reduce_lr],)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b374709",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu',\n",
    "input_shape=(64, 64, 3)))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(512, activation='relu'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Dropout(0.2))\n",
    "model.add(layers.Dense(2, activation='softmax'))\n",
    "\n",
    "opt = keras.optimizers.SGD(learning_rate=1e-2)\n",
    "model.compile(loss='binary_crossentropy',optimizer=opt,metrics=['acc','AUC'])\n",
    "\n",
    "# callbacks\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\n",
    "    r\"~\\models\\3convBNdroplr1e-2.h5\", save_best_only=True\n",
    ")\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(monitor=\"val_acc\", patience=15)\n",
    "\n",
    "reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
    "                              patience=5, min_lr=1e-4)\n",
    "\n",
    "start_time = time.time()\n",
    "history = model.fit_generator(train_generator,steps_per_epoch=100,epochs=80,validation_data=validation_generator,validation_steps=50,shuffle=True,callbacks=[checkpoint_cb, early_stopping_cb,reduce_lr],)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49027357",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(64, 64, 3)))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(512, activation='relu'))\n",
    "model.add(layers.Dense(2, activation='softmax'))\n",
    "\n",
    "\n",
    "opt = keras.optimizers.SGD(learning_rate=1e-2)\n",
    "model.compile(loss='binary_crossentropy',optimizer=opt,metrics=['acc','AUC'])\n",
    "\n",
    "# callbacks\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\n",
    "    r\"~\\models\\4convlr1e-2.h5\", save_best_only=True\n",
    ")\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(monitor=\"val_acc\", patience=15)\n",
    "\n",
    "reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
    "                              patience=5, min_lr=1e-4)\n",
    "\n",
    "start_time = time.time()\n",
    "history = model.fit_generator(train_generator,steps_per_epoch=100,epochs=80,validation_data=validation_generator,validation_steps=50,shuffle=True,callbacks=[checkpoint_cb, early_stopping_cb,reduce_lr],)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3d9a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(64, 64, 3)))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(512, activation='relu'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Dense(2, activation='softmax'))\n",
    "\n",
    "opt = keras.optimizers.SGD(learning_rate=1e-2)\n",
    "model.compile(loss='binary_crossentropy',optimizer=opt,metrics=['acc','AUC'])\n",
    "\n",
    "# callbacks\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\n",
    "    r\"~\\models\\4convBNlr1e-2.h5\", save_best_only=True\n",
    ")\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(monitor=\"val_acc\", patience=15)\n",
    "\n",
    "reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
    "                              patience=5, min_lr=1e-4)\n",
    "\n",
    "start_time = time.time()\n",
    "history = model.fit_generator(train_generator,steps_per_epoch=100,epochs=80,validation_data=validation_generator,validation_steps=50,shuffle=True,callbacks=[checkpoint_cb, early_stopping_cb,reduce_lr],)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b31d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(64, 64, 3)))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(512, activation='relu'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Dropout(0.2))\n",
    "model.add(layers.Dense(2, activation='softmax'))\n",
    "\n",
    "opt = keras.optimizers.SGD(learning_rate=1e-2)\n",
    "model.compile(loss='binary_crossentropy',optimizer=opt,metrics=['acc','AUC'])\n",
    "\n",
    "# callbacks\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\n",
    "    r\"~\\models\\4convBNdroplr1e-2.h5\", save_best_only=True\n",
    ")\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(monitor=\"val_acc\", patience=15)\n",
    "\n",
    "reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
    "                              patience=5, min_lr=1e-4)\n",
    "\n",
    "start_time = time.time()\n",
    "history = model.fit_generator(train_generator,steps_per_epoch=100,epochs=80,validation_data=validation_generator,validation_steps=50,shuffle=True,callbacks=[checkpoint_cb, early_stopping_cb,reduce_lr],)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483cf399",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots\n",
    "import matplotlib.pyplot as plt\n",
    "accuracy = history.history[\"acc\"]\n",
    "val_accuracy = history.history[\"val_acc\"]\n",
    "loss = history.history[\"loss\"]\n",
    "val_loss = history.history[\"val_loss\"]\n",
    "auc=history.history['auc']\n",
    "val_auc=history.history['val_auc']\n",
    "epochs = range(1, len(accuracy) + 1)\n",
    "plt.plot(epochs, accuracy, \"bo\", label=\"Training accuracy\")\n",
    "plt.plot(epochs, val_accuracy, \"b\", label=\"Validation accuracy\")\n",
    "plt.title(\"Training and validation accuracy\")\n",
    "plt.legend()\n",
    "plt.figure() \n",
    "plt.plot(epochs, loss, \"bo\", label=\"Training loss\")\n",
    "plt.plot(epochs, val_loss, \"b\", label=\"Validation loss\")\n",
    "plt.title(\"Training and validation loss\")\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(epochs, auc, \"bo\", label=\"training AUC\")\n",
    "plt.plot(epochs, val_auc, \"b\", label=\"Validation AUC\")\n",
    "plt.title(\"AUC curve\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4e2e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "from keras.models import load_model\n",
    "import os\n",
    "import numpy as np\n",
    "import keras.utils as image\n",
    "model = load_model(r'~\\models\\4convlr1e-2.h5') # loading the model\n",
    "\n",
    "test_path=r'~\\preproc_2d_data\\test'\n",
    "label_names=['AD','CN']\n",
    "actual_labels={}\n",
    "pred_labels={}\n",
    "# decision algorithm for image-level performance\n",
    "for i in label_names:\n",
    "    pred_AD=0\n",
    "    pred_CN=0 \n",
    "    for j in os.listdir(test_path+'\\\\'+str(i)): # j is sub-n\n",
    "        actual_labels[j]=i\n",
    "        for k in os.listdir(test_path+'\\\\'+str(i)+'\\\\'+str(j)):\n",
    "            testimage = image.load_img(test_path+'\\\\'+str(i)+'\\\\'+str(j)+'\\\\'+str(k), target_size=(64, 64))\n",
    "            testimage = image.img_to_array(testimage)\n",
    "            testimage = testimage/255\n",
    "            testimage = testimage.reshape(1, 64, 64, 3)\n",
    "            if label_names[np.argmax(model.predict(testimage))]=='AD': \n",
    "                pred_AD+=1 # if the model predicts AD for the k image we add 1 to pred_AD\n",
    "            elif label_names[np.argmax(model.predict(testimage))]=='CN':\n",
    "                pred_CN+=1 # if the model predicts CN for the k image we add 1 to pred_CN\n",
    "        if pred_AD>=pred_CN: # having predicted for all the images, we now predict the label for the subject with the voting process\n",
    "            pred_labels[j]='AD' # if the majority of pictures were predictes to be AD, we assign AD to the j's predicted label\n",
    "        elif pred_AD<pred_CN:\n",
    "            pred_labels[j]='CN'\n",
    "\n",
    "'''Accuracy'''\n",
    "N=0\n",
    "correct=0\n",
    "for i in list(actual_labels.keys()):\n",
    "    N+=1\n",
    "    if actual_labels[i]==pred_labels[i]:\n",
    "        correct+=1\n",
    "\n",
    "'''Precision, Recall & F1 score'''\n",
    "TP_ad=0\n",
    "FP_ad=0\n",
    "FN_ad=0\n",
    "TN_ad=0\n",
    "for i in list(actual_labels.keys()):\n",
    "    if actual_labels[i]=='AD' and pred_labels[i]=='AD':\n",
    "        TP_ad+=1\n",
    "    elif actual_labels[i]=='AD' and pred_labels[i]=='CN':\n",
    "        FN_ad+=1\n",
    "    elif actual_labels[i]=='CN' and pred_labels[i]=='CN':\n",
    "        TN_ad+=1\n",
    "    elif actual_labels[i]=='CN' and pred_labels[i]=='AD':\n",
    "        FP_ad+=1\n",
    "\n",
    "# Save metrics to a CSV file\n",
    "metrics = {\n",
    "    'Accuracy': correct/N,\n",
    "    'Precision for AD class': 0,\n",
    "    'Recall for AD class': 0,\n",
    "    'Precision for CN class': 0,\n",
    "    'Recall for CN class': 0,\n",
    "    'F1 score': 0\n",
    "}\n",
    "\n",
    "try:\n",
    "    metrics['Precision for AD class'] = TP_ad/(TP_ad+FP_ad)\n",
    "    metrics['Recall for AD class'] = TP_ad/(TP_ad+FN_ad)\n",
    "    metrics['Precision for CN class'] = TN_ad/(TN_ad+FN_ad)\n",
    "    metrics['Recall for CN class'] = TN_ad/(TN_ad+FP_ad), 2\n",
    "    metrics['F1 score'] = (2*((TP_ad/(TP_ad+FP_ad))*(TP_ad/(TP_ad+FN_ad))))/(TN_ad/(TN_ad+FN_ad)+TN_ad/(TN_ad+FP_ad))\n",
    "except ZeroDivisionError:\n",
    "    print('Error: division by zero')\n",
    "\n",
    "with open(r'~\\metrics.csv', mode='w') as csv_file:\n",
    "    fieldnames = metrics.keys()\n",
    "    writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "\n",
    "    writer.writeheader()\n",
    "    writer.writerow(metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22bed89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''3D CNNs'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0019e8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import keras.utils as image\n",
    "import gc\n",
    "import csv\n",
    "\n",
    "\n",
    "'''To process the data, we do the following:\n",
    "\n",
    "We first rotate the volumes by 90 degrees, so the orientation is fixed\n",
    "We scale the HU values to be between 0 and 1.\n",
    "We resize width, height and depth.'''\n",
    "\n",
    "# some functions for our preprocessing. In this case, I will not use the resize_volume but i keep it for possible future use\n",
    "import nibabel as nib\n",
    "\n",
    "from scipy import ndimage\n",
    "\n",
    "\n",
    "def read_nifti_file(filepath):\n",
    "    \"\"\"Read and load volume\"\"\"\n",
    "    # Read file\n",
    "    scan = nib.load(filepath)\n",
    "    # Get raw data\n",
    "    scan = scan.get_fdata()\n",
    "    return scan\n",
    "\n",
    "\n",
    "def normalize(volume):\n",
    "    \"\"\"Normalize the volume\"\"\"\n",
    "    min = 0.0\n",
    "    max = 38000.0 \n",
    "    volume[volume < min] = min\n",
    "    volume[volume > max] = max\n",
    "    volume = (volume - min) / (max - min)\n",
    "    volume = volume.astype(\"float32\")\n",
    "    return volume\n",
    "\n",
    "\n",
    "def resize_volume(img):\n",
    "    \"\"\"Resize across z-axis\"\"\"\n",
    "    # Set the desired depth\n",
    "    desired_depth = 48\n",
    "    desired_width = 64\n",
    "    desired_height = 64\n",
    "    # Get current depth\n",
    "    current_depth = img.shape[-1]\n",
    "    current_width = img.shape[0]\n",
    "    current_height = img.shape[1]\n",
    "    # Compute depth factor\n",
    "    depth = current_depth / desired_depth\n",
    "    width = current_width / desired_width\n",
    "    height = current_height / desired_height\n",
    "    depth_factor = 1 / depth\n",
    "    width_factor = 1 / width\n",
    "    height_factor = 1 / height\n",
    "    # Rotate\n",
    "    #img = ndimage.rotate(img, 90, reshape=False) # i skip 90 degree rotation in my algorithm\n",
    "    # Resize across z-axis\n",
    "    img = ndimage.zoom(img, (width_factor, height_factor, depth_factor), order=1)\n",
    "    return img\n",
    "\n",
    "\n",
    "def process_scan(path):\n",
    "    \"\"\"Read and resize volume\"\"\"\n",
    "    # Read scan\n",
    "    volume = read_nifti_file(path)\n",
    "    # Normalize\n",
    "    volume = normalize(volume)\n",
    "    # Resize width, height and depth\n",
    "    volume = resize_volume(volume)\n",
    "    return volume\n",
    "\n",
    "    # Read the paths of the MRI scans from the class directories.\n",
    "normal_scan_paths_train = [\n",
    "    os.path.join(os.getcwd(), r'~/train/CN', x)\n",
    "    for x in os.listdir(r'~/train/CN')\n",
    "]\n",
    "\n",
    "abnormal_scan_paths_train = [\n",
    "    os.path.join(os.getcwd(), r'~/train/AD', x)\n",
    "    for x in os.listdir(r'~/train/AD')\n",
    "]\n",
    "\n",
    "normal_scan_paths_val = [\n",
    "    os.path.join(os.getcwd(), r'~/validation/CN', x)\n",
    "    for x in os.listdir(r'~/validation/CN')\n",
    "]\n",
    "\n",
    "abnormal_scan_paths_val = [\n",
    "    os.path.join(os.getcwd(), r'~/validation/AD', x)\n",
    "    for x in os.listdir(r'~/validation/AD')\n",
    "]\n",
    "\n",
    "\n",
    "# Shuffle the image paths randomly.\n",
    "np.random.shuffle(normal_scan_paths_train)\n",
    "np.random.shuffle(abnormal_scan_paths_train)\n",
    "np.random.shuffle(normal_scan_paths_val)\n",
    "np.random.shuffle(abnormal_scan_paths_val)\n",
    "\n",
    "normal_scan_paths_train = normal_scan_paths_train[:int(len(normal_scan_paths_train)*0.5)]\n",
    "abnormal_scan_paths_train = abnormal_scan_paths_train[:int(len(abnormal_scan_paths_train)*0.5)]\n",
    "normal_scan_paths_val = normal_scan_paths_val[:int(len(normal_scan_paths_val)*0.5)]\n",
    "abnormal_scan_paths_val = abnormal_scan_paths_val[:int(len(abnormal_scan_paths_val)*0.5)]\n",
    "\n",
    "print(\"Number of MRIs for training from cognitively normal: \" + str(len(normal_scan_paths_train)))\n",
    "print(\"Number of MRIs for training from Alzheimer's Disease: \" + str(len(abnormal_scan_paths_train)))\n",
    "print(\"Number of MRIs for validation from cognitively normal: \" + str(len(normal_scan_paths_val)))\n",
    "print(\"Number of MRIs for validation from Alzheimer's Disease: \" + str(len(abnormal_scan_paths_val)))\n",
    "\n",
    "\n",
    "# Read and process the scans.\n",
    "# Each scan is resized across height, width, and depth and rescaled.\n",
    "train_abnormal_scans = np.array([process_scan(path) for path in abnormal_scan_paths_train])\n",
    "train_normal_scans = np.array([process_scan(path) for path in normal_scan_paths_train])\n",
    "val_abnormal_scans = np.array([process_scan(path) for path in abnormal_scan_paths_val])\n",
    "val_normal_scans = np.array([process_scan(path) for path in normal_scan_paths_val])\n",
    "# For the fMRIs of AD patients assign 1, for the normal ones assign 0.\n",
    "train_abnormal_labels = np.array([1 for _ in range(len(abnormal_scan_paths_train))])\n",
    "train_normal_labels = np.array([0 for _ in range(len(normal_scan_paths_train))])\n",
    "val_abnormal_labels = np.array([1 for _ in range(len(abnormal_scan_paths_val))])\n",
    "val_normal_labels = np.array([0 for _ in range(len(normal_scan_paths_val))])\n",
    "# Split data in the ratio 70-30 for training and validation.\n",
    "x_train = np.concatenate((train_abnormal_scans[:], train_normal_scans[:]), axis=0)\n",
    "y_train = np.concatenate((train_abnormal_labels[:], train_normal_labels[:]), axis=0)\n",
    "x_val = np.concatenate((val_abnormal_scans[:], val_normal_scans[:]), axis=0)\n",
    "y_val = np.concatenate((val_abnormal_labels[:], val_normal_labels[:]), axis=0)\n",
    "print(\n",
    "    \"Number of samples in train and validation are %d and %d.\"\n",
    "    % (x_train.shape[0], x_val.shape[0])\n",
    ")\n",
    "\n",
    "del normal_scan_paths_train\n",
    "del abnormal_scan_paths_train\n",
    "del normal_scan_paths_val\n",
    "del abnormal_scan_paths_val\n",
    "del train_abnormal_scans\n",
    "del train_normal_scans\n",
    "del val_abnormal_scans\n",
    "del val_normal_scans\n",
    "del train_abnormal_labels\n",
    "del train_normal_labels\n",
    "del val_abnormal_labels\n",
    "del val_normal_labels\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data into train and test sets.\n",
    "x_train1, x_train2, y_train1, y_train2 = train_test_split(x_train, y_train, test_size=0.99, random_state=42)\n",
    "x_val1, x_val2, y_val1, y_val2 = train_test_split(x_val, y_val, test_size=0.99, random_state=42)\n",
    "\n",
    "print(\"Number of samples in first train set and second train set are %d and %d.\" % (x_train1.shape[0], x_train2.shape[0]))\n",
    "print(\"Number of samples in validation set 1 and 2 are %d and %d.\" % (x_val1.shape[0], x_val2.shape[0]))\n",
    "\n",
    "del x_train\n",
    "del y_train\n",
    "del x_val\n",
    "del y_val\n",
    "del x_train1\n",
    "del y_train1\n",
    "del x_val1\n",
    "del y_val1\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "import random\n",
    "\n",
    "from scipy import ndimage\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def rotate(volume):\n",
    "    \"\"\"Rotate the volume by a few degrees\"\"\"\n",
    "\n",
    "    def scipy_rotate(volume):\n",
    "        # define some rotation angles\n",
    "        angles = [-20, -10, -5, 5, 10, 20]\n",
    "        # pick angles at random\n",
    "        angle = random.choice(angles)\n",
    "        # rotate volume\n",
    "        volume = ndimage.rotate(volume, angle, reshape=False)\n",
    "        volume[volume < 0] = 0\n",
    "        volume[volume > 1] = 1\n",
    "        return volume\n",
    "\n",
    "    augmented_volume = tf.numpy_function(scipy_rotate, [volume], tf.float32)\n",
    "    return augmented_volume\n",
    "\n",
    "\n",
    "def train_preprocessing(volume, label):\n",
    "    \"\"\"Process training data by rotating and adding a channel.\"\"\"\n",
    "    # Rotate volume\n",
    "    volume = rotate(volume)\n",
    "    volume = tf.expand_dims(volume, axis=3)\n",
    "    return volume, label\n",
    "\n",
    "\n",
    "def validation_preprocessing(volume, label):\n",
    "    \"\"\"Process validation data by only adding a channel.\"\"\"\n",
    "    volume = tf.expand_dims(volume, axis=3)\n",
    "    return volume, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe6f403",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data loaders.\n",
    "train_loader = tf.data.Dataset.from_tensor_slices((x_train2, y_train2))\n",
    "validation_loader = tf.data.Dataset.from_tensor_slices((x_val2, y_val2))\n",
    "\n",
    "batch_size = 20\n",
    "# Augment the on the fly during training.\n",
    "train_dataset = (\n",
    "    train_loader.shuffle(len(x_train2))\n",
    "    .map(train_preprocessing)\n",
    "    .batch(batch_size)\n",
    "    .prefetch(2)\n",
    ")\n",
    "# Only rescale.\n",
    "validation_dataset = (\n",
    "    validation_loader.shuffle(len(x_val2))\n",
    "    .map(validation_preprocessing)\n",
    "    .batch(batch_size)\n",
    "    .prefetch(2)\n",
    ")\n",
    "\n",
    "del x_train2\n",
    "del y_train2\n",
    "del x_val2\n",
    "del y_val2\n",
    "gc.collect()\n",
    "\n",
    "from keras import layers\n",
    "from keras import models\n",
    "\n",
    "model = models.Sequential()\n",
    "model = models.Sequential()\n",
    "# Block 1\n",
    "model.add(layers.Conv3D(32, (3, 3, 3), activation='relu', padding='same',input_shape=(64, 64, 48, 1)))\n",
    "model.add(layers.MaxPooling3D((2, 2, 2)))\n",
    "# Block 2\n",
    "model.add(layers.Conv3D(64, (3, 3, 3), activation='relu', padding='same'))\n",
    "model.add(layers.MaxPooling3D((2, 2, 2)))\n",
    "# Block 3\n",
    "model.add(layers.Conv3D(128, (3, 3, 3), activation='relu', padding='same'))\n",
    "model.add(layers.MaxPooling3D((2, 2, 2)))\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(512, activation='relu'))\n",
    "model.add(layers.Dense(2, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "opt = keras.optimizers.SGD(learning_rate=1e-2)\n",
    "model.compile(loss='binary_crossentropy',optimizer=opt,metrics=['acc','AUC'])\n",
    "\n",
    "# callbacks\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\n",
    "    r\"~\\models\\3D_3convlr1e-2.h5\", save_best_only=True\n",
    ")\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(monitor=\"val_acc\", patience=15)\n",
    "\n",
    "reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
    "                              patience=5, min_lr=1e-4)\n",
    "\n",
    "start_time = time.time()\n",
    "history = model.fit_generator(train_generator,steps_per_epoch=100,epochs=80,validation_data=validation_generator,validation_steps=50,shuffle=True,callbacks=[checkpoint_cb, early_stopping_cb,reduce_lr],)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "# plots\n",
    "import matplotlib.pyplot as plt\n",
    "accuracy = history.history[\"accuracy\"]\n",
    "val_accuracy = history.history[\"val_accuracy\"]\n",
    "loss = history.history[\"loss\"]\n",
    "val_loss = history.history[\"val_loss\"]\n",
    "epochs = range(1, len(accuracy) + 1)\n",
    "plt.plot(epochs, accuracy, \"bo\", label=\"Training accuracy\")\n",
    "plt.plot(epochs, val_accuracy, \"b\", label=\"Validation accuracy\")\n",
    "plt.title(\"Training and validation accuracy\")\n",
    "plt.legend()\n",
    "plt.figure() \n",
    "plt.plot(epochs, loss, \"bo\", label=\"Training loss\")\n",
    "plt.plot(epochs, val_loss, \"b\", label=\"Validation loss\")\n",
    "plt.title(\"Training and validation loss\")\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ca528d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation\n",
    "model = load_model(r'~\\models\\3D_3convlr1e-2.h5') # loading the model\n",
    "test_path=r'~/test'\n",
    "label_names=['CN','AD']\n",
    "actual_labels={}\n",
    "pred_labels={}\n",
    "for i in label_names:\n",
    "    pred_AD=0\n",
    "    pred_CN=0 \n",
    "    for j in os.listdir(test_path+'/'+str(i)): # j is sub-n\n",
    "        actual_labels[j]=i\n",
    "        for k in os.listdir(test_path+'/'+str(i)+'/'+str(j)):\n",
    "            image=process_scan(test_path+'/'+str(i)+'/'+str(j)+'/'+str(k))\n",
    "            image=image.reshape(1, 64, 64, 48, 1)\n",
    "            if label_names[np.argmax(model.predict(image))]=='AD': \n",
    "                pred_AD+=1 # if the model predicts AD for the k image we add 1 to pred_AD\n",
    "            elif label_names[np.argmax(model.predict(image))]=='CN':\n",
    "                pred_CN+=1 # if the model predicts CN for the k image we add 1 to pred_CN\n",
    "        if pred_AD>=pred_CN: # having predicted for all the images, we now predict the label for the subject with the voting process\n",
    "            pred_labels[j]='AD' # if the majority of pictures were predictes to be AD, we assign AD to the j's predicted label\n",
    "        elif pred_AD<pred_CN:\n",
    "            pred_labels[j]='CN'\n",
    "\n",
    "'''Accuracy'''\n",
    "N=0\n",
    "correct=0\n",
    "for i in list(actual_labels.keys()):\n",
    "    N+=1\n",
    "    if actual_labels[i]==pred_labels[i]:\n",
    "        correct+=1\n",
    "\n",
    "'''Precision, Recall & F1 score'''\n",
    "TP_ad=0\n",
    "FP_ad=0\n",
    "FN_ad=0\n",
    "TN_ad=0\n",
    "for i in list(actual_labels.keys()):\n",
    "    if actual_labels[i]=='AD' and pred_labels[i]=='AD':\n",
    "        TP_ad+=1\n",
    "    elif actual_labels[i]=='AD' and pred_labels[i]=='CN':\n",
    "        FN_ad+=1\n",
    "    elif actual_labels[i]=='CN' and pred_labels[i]=='CN':\n",
    "        TN_ad+=1\n",
    "    elif actual_labels[i]=='CN' and pred_labels[i]=='AD':\n",
    "        FP_ad+=1\n",
    "\n",
    "metrics = {\n",
    "    'Accuracy': correct/N,\n",
    "    'Precision for AD class': 0,\n",
    "    'Recall for AD class': 0,\n",
    "    'Precision for CN class': 0,\n",
    "    'Recall for CN class': 0,\n",
    "    'F1 score': 0\n",
    "}\n",
    "try:\n",
    "    metrics['Precision for AD class'] = TP_ad/(TP_ad+FP_ad)\n",
    "    metrics['Recall for AD class'] = TP_ad/(TP_ad+FN_ad)\n",
    "    metrics['Precision for CN class'] = TN_ad/(TN_ad+FN_ad)\n",
    "    metrics['Recall for CN class'] = TN_ad/(TN_ad+FP_ad), 2\n",
    "    metrics['F1 score'] = (2*((TP_ad/(TP_ad+FP_ad))*(TP_ad/(TP_ad+FN_ad))))/(TN_ad/(TN_ad+FN_ad)+TN_ad/(TN_ad+FP_ad))\n",
    "except ZeroDivisionError:\n",
    "    print('Error: division by zero')\n",
    "print(metrics)\n",
    "\n",
    "with open(r'~\\metrics.csv', mode='w') as csv_file:\n",
    "    fieldnames = metrics.keys()\n",
    "    writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "\n",
    "    writer.writeheader()\n",
    "    writer.writerow(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db3972a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the rest of the 3D architectures follow in a similar fashion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c77cf48",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
